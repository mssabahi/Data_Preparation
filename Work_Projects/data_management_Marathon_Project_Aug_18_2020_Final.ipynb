{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from operator import add\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.width', 1000)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pylab\n",
    "from datetime import datetime\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ssabahi\\\\Desktop\\\\Marathon\\\\20-2722\\\\EMMA_Prepration\\\\Excel_01'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# file_names = sorted(glob.glob('*.xlsx'))\n",
    "file_names = (glob.glob('*.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Excel_01.xlsx',\n",
       " 'GW_5_EMMA Ready.xlsx',\n",
       " 'Status_Original.xlsx',\n",
       " '~$Excel_01.xlsx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_dict = pd.read_excel('./'+file_names[0], sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheetname_ls = list(sheets_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sheetname_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the borehole data"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheetname_noBH_ls = sheetname_ls[:72]\n",
    "# sheetname_noBH_ls = sheetname_noBH_ls[:5] # For Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = pd.DataFrame(columns=['Sheet_Name','Parameters_Needed','Parameters+Units_Needed',\n",
    "                                 'Parameters+Units_Needed_ls_Dimension','Parameters_Needed_ls_Dimension','Difference',\n",
    "                                 'Sheet_df_Dimension'], index=list(range(1,len(sheetname_noBH_ls)+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the Parameters and Units needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_ls = []\n",
    "temp_parunit_ls = []\n",
    "df = []\n",
    "for i,j in enumerate(sheetname_noBH_ls):\n",
    "    temp_df = pd.read_excel('./' + file_names[0],sheet_name = j,skiprows=6)\n",
    "    temp_df['Parameters'] = temp_df['Parameters'].apply(lambda row: str(row).strip())\n",
    "    temp_df = temp_df.mask(lambda x: x == '-')\n",
    "    temp_df = temp_df.mask(lambda x: x == '*')\n",
    "    \n",
    "    \n",
    "#     temp_df['Parameters'] = temp_df['Parameters'].apply(lambda row: row[:-10] if row[-10:]=='-Dissolved' else row[:-9] if row[-9:]=='Dissolved' else row )\n",
    "    temp_df.loc[temp_df['Parameters'].str.contains('Nitrate'), 'Parameters'] = 'Nitrate-N (NO3-N)'\n",
    "    temp_df.loc[temp_df['Parameters'].str.contains('Nitrite'), 'Parameters'] = 'Nitrite-N (NO2-N)'\n",
    "    temp_df.loc[temp_df['Parameters'].str.contains('Aluminum (Al) - Dissolved'), 'Parameters'] = 'Aluminum (Al)-Dissolved'\n",
    "    temp_df.loc[temp_df['Parameters'].str.contains('Hardness'), 'Parameters'] = 'Hardness (as CaCO3)'\n",
    "    temp_df.loc[temp_df['Parameters'].str.contains('Fluoride'), 'Parameters'] = 'Fluoride (F)'\n",
    "    temp_df.loc[temp_df['Parameters'].str.contains('Mercury (Hg) - Dissolved'), 'Parameters'] = 'Mercury (Hg)-Dissolved'\n",
    "\n",
    "\n",
    "#     temp_df.loc[temp_df['Parameters'].str.contains('Phosphorus'), 'Parameters'] = 'Phosphorus (P)-Total'\n",
    "#     temp_df.loc[temp_df['Parameters'].str.contains('Lithium'), 'Parameters'] = 'Lithium (Li)'\n",
    "    temp_df.loc[temp_df['Parameters'].str.contains('Conductivity'), 'Parameters'] = 'Conductivity (EC)'\n",
    "    temp_df.loc[temp_df['Parameters'].str.contains('Oxygen Demand'), 'Parameters'] = 'Chemical Oxygen Demand'\n",
    "    temp_df['Units'] = temp_df['Units'].replace(to_replace= r'0C', value='Celsius', regex=True)\n",
    "    temp_df['Units'] = temp_df['Units'].replace(to_replace= r'°C', value='Celsius', regex=True)\n",
    "    \n",
    "    temp_df['Units'] = temp_df['Units'].replace(to_replace= r'ug/L', value='µg/L', regex=True)\n",
    "    temp_df['Units'] = temp_df['Units'].replace(to_replace= r'uS/cm', value='µS/cm', regex=True)\n",
    "\n",
    "    \n",
    "    # ls1 = list(temp_df['Parameters'])\n",
    "    temp_df_1 = temp_df.dropna(subset=['Units'])\n",
    "    ls2 = list(temp_df_1['Parameters'])\n",
    "    # par_ls = list(set(ls1)-set(ls2))\n",
    "    \n",
    "    par_unit_ls = list(temp_df_1['Parameters'] + ' + ' + temp_df_1['Units'])\n",
    "#     par_unit_ls = [x for x in mm if str(x) != 'nan']\n",
    "\n",
    "    temp_df['Note'] = 'Sheet_Name =' + j\n",
    "      \n",
    "\n",
    "    df.append(temp_df)\n",
    "    temp_ls.append(ls2)\n",
    "    temp_parunit_ls.append(par_unit_ls)\n",
    "    \n",
    "\n",
    "    \n",
    "final_ls = []\n",
    "final_parunit_ls = []\n",
    "for i in range(len(temp_ls)):\n",
    "    final_ls = list(set(final_ls + list(temp_ls[i]))) \n",
    "    final_parunit_ls = list(set(final_parunit_ls + list(temp_parunit_ls[i]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(sheetname_noBH_ls):\n",
    "    status_df.iloc[i]['Sheet_Name'] = j\n",
    "    ttemp_df = df[i].dropna(subset=['Units'])\n",
    "                              \n",
    "    par_ls = list( ttemp_df['Parameters'] )\n",
    "    unit_ls = list(ttemp_df['Units'])\n",
    "    par_unit_ls = list(ttemp_df['Parameters'] + ' + ' + ttemp_df['Units'])\n",
    "\n",
    "    par_needed_ls = list(set(final_ls) - set(par_ls) ) + list(set(par_ls) - set(final_ls))\n",
    "    par_unit_needed_ls = list(set(final_parunit_ls) - set(par_unit_ls) ) + list(set(par_unit_ls) - set(final_parunit_ls))\n",
    "    par_unit_needed_ls2 = [x.split(' +')[0] for x in (par_unit_needed_ls) ]\n",
    "    \n",
    "    status_df.iloc[i]['Parameters_Needed'] = par_needed_ls\n",
    "    status_df.iloc[i]['Parameters+Units_Needed'] = par_unit_needed_ls\n",
    "    \n",
    "    status_df.iloc[i]['Parameters+Units_Needed_ls_Dimension'] = len(par_unit_needed_ls)\n",
    "    status_df.iloc[i]['Parameters_Needed_ls_Dimension'] = len(par_needed_ls)\n",
    "    \n",
    "    status_df.iloc[i]['Difference'] = list( set(par_unit_needed_ls2) - set(par_needed_ls) )\n",
    "    status_df.iloc[i]['Sheet_df_Dimension'] = df[i].shape\n",
    "    \n",
    "    \n",
    "    ss = pd.DataFrame(columns = df[i].columns, index=range(0, len(par_unit_needed_ls)) )\n",
    "\n",
    "    for k, l in enumerate(par_unit_needed_ls):\n",
    "        ss.iloc[k,0] = l.split(' + ')[0]\n",
    "        ss.iloc[k,1] = l.split(' + ')[1]\n",
    "    \n",
    "    \n",
    "    df[i] = df[i].append(ss).reset_index(drop=True)"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_cols = ['Site_Name', 'Media_Name', 'Lab_Track_ID', 'Location_Name', 'Sample_ID',\n",
    "       'Lab_Sample_ID', 'Duplicate_Source_ID', 'Duplicate_Lab_Source_ID',\n",
    "       'Is_Travel_Blank', 'Is_Field_Blank', 'Depth_Start_M', 'Depth_End_M',\n",
    "       'Diluted', 'Sample_DateTime', 'Analysis_DateTime', 'Analysis_Method',\n",
    "       'Parameter_ID', 'Sample_Value', 'Less_Than_DL', 'Detection_Limit',\n",
    "       'Units', 'Spike_Recovery', 'Species', 'Sample_Treatment',\n",
    "       'Soil_Texture', 'Notes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_master_df_EMMA = pd.DataFrame(columns=emma_cols)\n",
    "for i , j in enumerate(df):\n",
    "    mm = df[i]\n",
    "    mm.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    mm_header = mm.iloc[:3,:]\n",
    "    mm_footer = mm.iloc[3:,:]\n",
    "    mm_footer.dropna(axis=1, how='all', inplace=True)\n",
    "    para_df = mm_footer.iloc[:,4:-1]\n",
    "    \n",
    "    ####################################################################\n",
    "    \n",
    "    Sample_DateTime_ls = list(mm_header.iloc[0,4:-1])\n",
    "    Sample_ID_ls = list(mm_header.iloc[1,4:-1])\n",
    "    \n",
    "    for ii, jj in enumerate(Sample_ID_ls):\n",
    "            if 'rep' in (Sample_ID_ls[ii].lower()):\n",
    "                Sample_ID_ls[ii] = 'Dup+' + Sample_ID_ls[ii-1]\n",
    "\n",
    "\n",
    "    Sample_Value_df = pd.DataFrame(pd.Series(para_df.values.ravel('F')),columns = ['Sample_Value'] )\n",
    "\n",
    "    Units_df = pd.DataFrame(mm_footer['Units'] )\n",
    "    Units_df.columns = ['Units']\n",
    "    Units_df = pd.concat( [Units_df] * int(Sample_Value_df.size/mm_footer.shape[0]) )\n",
    "    Units_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    Parameter_ID_df = pd.DataFrame(mm_footer['Parameters'] )\n",
    "    Parameter_ID_df.columns = ['Parameter_ID']\n",
    "    Parameter_ID_df = pd.concat( [Parameter_ID_df] * int(Sample_Value_df.size/mm_footer.shape[0]) )\n",
    "    Parameter_ID_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "    Sample_ID_df = pd.DataFrame(list(itertools.chain.from_iterable(itertools.repeat(x, mm_footer.shape[0])\n",
    "                                      for x in Sample_ID_ls)) ,\n",
    "                                      columns = ['Lab_Sample_ID'])\n",
    "\n",
    "    Sample_DateTime_df = pd.DataFrame(list(itertools.chain.from_iterable(itertools.repeat(x, mm_footer.shape[0])\n",
    "                                  for x in Sample_DateTime_ls)) ,\n",
    "                                  columns = ['Sample_DateTime'])\n",
    "    \n",
    "    final_df = pd.concat([Parameter_ID_df, Sample_Value_df, Sample_ID_df, Units_df, Sample_DateTime_df], axis=1, sort=False)\n",
    "\n",
    "    final_df['Location_Name'] = mm_header.columns[4]\n",
    "\n",
    "    \n",
    "    add_cols_ls = list(set(emma_cols) - set(list(final_df.columns)) )\n",
    "\n",
    "    \n",
    "    \n",
    "    for ii, jj  in enumerate(add_cols_ls):\n",
    "        final_df.insert(len(final_df.columns), jj,'', allow_duplicates=True)\n",
    "    \n",
    "    \n",
    "    final_df['Sample_DateTime'] = pd.to_datetime(final_df['Sample_DateTime'],errors='coerce')\n",
    "    final_df = final_df[emma_cols]\n",
    "    final_df = final_df.dropna(subset=['Units'])\n",
    "    \n",
    "    ########################################################################\n",
    "    out_path = \"C:\\\\Users\\\\ssabahi\\\\Desktop\\\\Marathon\\\\20-2722\\\\EMMA_Prepration\\\\Excel_01\\\\Final\\\\\" + mm_header.columns[4] + \".xlsx\"\n",
    "    writer = pd.ExcelWriter(out_path , engine='xlsxwriter')\n",
    "    final_df.to_excel(writer, sheet_name='EMMA' + '-' + mm_header.columns[4], index=False)\n",
    "    writer.save()\n",
    "    ########################################################################\n",
    "    \n",
    "    final_master_df_EMMA = final_master_df_EMMA.append(final_df)\n",
    "    final_master_df_EMMA.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_master_df_EMMA['Site_Name'] = 'Marathon'\n",
    "\n",
    "final_master_df_EMMA['Media_Name'] = 'Surface Water'\n",
    "\n",
    "final_master_df_EMMA['Sample_ID'] = final_master_df_EMMA['Location_Name'] + ' - ' + final_master_df_EMMA['Sample_DateTime'].dt.strftime('%Y/%b/%d')\n",
    "\n",
    "final_master_df_EMMA['Duplicate_Source_ID'] =  final_master_df_EMMA.apply(lambda row: str(row.Lab_Sample_ID).split('Dup+')[1] if 'Dup' in str(row['Lab_Sample_ID']) \n",
    "                       else row['Duplicate_Source_ID'], axis=1 )\n",
    "\n",
    "final_master_df_EMMA['Lab_Sample_ID'] = final_master_df_EMMA['Lab_Sample_ID'].apply(lambda row:str(row).split('Dup+')[1] \n",
    "                                            if 'Dup' in str(row) else row )\n",
    "\n",
    "final_master_df_EMMA.reset_index(inplace=True, drop=True) \n",
    "\n",
    "final_master_df_EMMA.Location_Name =  final_master_df_EMMA.Location_Name.apply(lambda row: row.replace('-', '') )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_master_df_EMMA.Duplicate_Lab_Source_ID = final_master_df_EMMA.Duplicate_Source_ID\n",
    "\n",
    "final_master_df_EMMA.Duplicate_Source_ID = final_master_df_EMMA.apply(lambda row:row.Sample_ID if row.Duplicate_Lab_Source_ID!=''\n",
    "                                                                      else row.Duplicate_Lab_Source_ID, axis=1)\n",
    "\n",
    "final_master_df_EMMA.Lab_Sample_ID = final_master_df_EMMA.apply(lambda row:'Duplicate' if row.Duplicate_Lab_Source_ID!=''\n",
    "                                                                      else row.Lab_Sample_ID, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_master_df_EMMA['Sample_Value'] = final_master_df_EMMA['Sample_Value'].replace('NM', np.NaN)\n",
    "\n",
    "final_master_df_EMMA_NONAN = final_master_df_EMMA.dropna(subset=['Sample_Value']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "final_master_df_EMMA_NONAN.Sample_Value = final_master_df_EMMA_NONAN.apply(lambda row:float(row.Sample_Value) if ( (type(row.Sample_Value)==str) and ('<' not in row.Sample_Value) )\n",
    "                                                                                                                else row.Sample_Value, axis=1)                                                                          \n",
    "                                                                                     \n",
    "final_master_df_EMMA_NONAN.Less_Than_DL = final_master_df_EMMA_NONAN.apply(lambda row:True if type(row.Sample_Value)==str \n",
    "                                                                           else row.Less_Than_DL, axis=1)                                                                           \n",
    "                                                                           \n",
    "final_master_df_EMMA_NONAN.Detection_Limit = final_master_df_EMMA_NONAN.apply(lambda row:float(row.Sample_Value.split('<')[1]) if type(row.Sample_Value)==str \n",
    "                                                                           else row.Detection_Limit, axis=1)                                                                           \n",
    "                                                                           \n",
    "final_master_df_EMMA_NONAN.Sample_Value = final_master_df_EMMA_NONAN.apply(lambda row:float(row.Sample_Value.split('<')[1]) if type(row.Sample_Value)==str \n",
    "                                                                           else row.Sample_Value, axis=1)                                                                           \n"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"C:\\\\Users\\\\ssabahi\\\\Desktop\\\\Marathon\\\\20-2722\\\\EMMA_Prepration\\\\Excel_01\\\\Final\\\\Ready_for_EMMA_Excel_01.xlsx\"\n",
    "writer = pd.ExcelWriter(out_path , engine='xlsxwriter')\n",
    "final_master_df_EMMA.to_excel(writer, sheet_name='Ready_for_EMMA_Excel_01', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"C:\\\\Users\\\\ssabahi\\\\Desktop\\\\Marathon\\\\20-2722\\\\EMMA_Prepration\\\\Excel_01\\\\Final\\\\Status.xlsx\"\n",
    "writer = pd.ExcelWriter(out_path , engine='xlsxwriter')\n",
    "status_df.to_excel(writer, sheet_name='Status', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"C:\\\\Users\\\\ssabahi\\\\Desktop\\\\Marathon\\\\20-2722\\\\EMMA_Prepration\\\\Excel_01\\\\Final\\\\Ready_for_EMMA_NONAN_Excel_01.xlsx\"\n",
    "writer = pd.ExcelWriter(out_path , engine='xlsxwriter')\n",
    "final_master_df_EMMA_NONAN.to_excel(writer, sheet_name='Ready_for_EMMA_NONAN_Excel_01', index=False)\n",
    "writer.save()"
   ]
  },

 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
